{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.1.1\n",
      "      /_/\n",
      "\n",
      "Using Python version 2.7.13 (default, Dec 20 2016 23:05:08)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "execfile(os.path.join(os.environ[\"SPARK_HOME\"], 'python/pyspark/shell.py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x10f5b3c50>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PACKAGE_EXTENSIONS',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__doc__',\n",
       " '__enter__',\n",
       " '__exit__',\n",
       " '__format__',\n",
       " '__getattribute__',\n",
       " '__getnewargs__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__module__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_accumulatorServer',\n",
       " '_active_spark_context',\n",
       " '_batchSize',\n",
       " '_callsite',\n",
       " '_checkpointFile',\n",
       " '_conf',\n",
       " '_dictToJavaMap',\n",
       " '_do_init',\n",
       " '_ensure_initialized',\n",
       " '_gateway',\n",
       " '_getJavaStorageLevel',\n",
       " '_initialize_context',\n",
       " '_javaAccumulator',\n",
       " '_jsc',\n",
       " '_jvm',\n",
       " '_lock',\n",
       " '_next_accum_id',\n",
       " '_pickled_broadcast_vars',\n",
       " '_python_includes',\n",
       " '_temp_dir',\n",
       " '_unbatched_serializer',\n",
       " 'accumulator',\n",
       " 'addFile',\n",
       " 'addPyFile',\n",
       " 'appName',\n",
       " 'applicationId',\n",
       " 'binaryFiles',\n",
       " 'binaryRecords',\n",
       " 'broadcast',\n",
       " 'cancelAllJobs',\n",
       " 'cancelJobGroup',\n",
       " 'defaultMinPartitions',\n",
       " 'defaultParallelism',\n",
       " 'dump_profiles',\n",
       " 'emptyRDD',\n",
       " 'environment',\n",
       " 'getConf',\n",
       " 'getLocalProperty',\n",
       " 'getOrCreate',\n",
       " 'hadoopFile',\n",
       " 'hadoopRDD',\n",
       " 'master',\n",
       " 'newAPIHadoopFile',\n",
       " 'newAPIHadoopRDD',\n",
       " 'parallelize',\n",
       " 'pickleFile',\n",
       " 'profiler_collector',\n",
       " 'pythonExec',\n",
       " 'pythonVer',\n",
       " 'range',\n",
       " 'runJob',\n",
       " 'sequenceFile',\n",
       " 'serializer',\n",
       " 'setCheckpointDir',\n",
       " 'setJobGroup',\n",
       " 'setLocalProperty',\n",
       " 'setLogLevel',\n",
       " 'setSystemProperty',\n",
       " 'show_profiles',\n",
       " 'sparkHome',\n",
       " 'sparkUser',\n",
       " 'startTime',\n",
       " 'statusTracker',\n",
       " 'stop',\n",
       " 'textFile',\n",
       " 'uiWebUrl',\n",
       " 'union',\n",
       " 'version',\n",
       " 'wholeTextFiles']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'pyspark-shell'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.appName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'local[*]'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on SparkContext in module pyspark.context object:\n",
      "\n",
      "class SparkContext(__builtin__.object)\n",
      " |  Main entry point for Spark functionality. A SparkContext represents the\n",
      " |  connection to a Spark cluster, and can be used to create L{RDD} and\n",
      " |  broadcast variables on that cluster.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __enter__(self)\n",
      " |      Enable 'with SparkContext(...) as sc: app(sc)' syntax.\n",
      " |  \n",
      " |  __exit__(self, type, value, trace)\n",
      " |      Enable 'with SparkContext(...) as sc: app' syntax.\n",
      " |      \n",
      " |      Specifically stop the context on exit of the with block.\n",
      " |  \n",
      " |  __getnewargs__(self)\n",
      " |  \n",
      " |  __init__(self, master=None, appName=None, sparkHome=None, pyFiles=None, environment=None, batchSize=0, serializer=PickleSerializer(), conf=None, gateway=None, jsc=None, profiler_cls=<class 'pyspark.profiler.BasicProfiler'>)\n",
      " |      Create a new SparkContext. At least the master and app name should be set,\n",
      " |      either through the named parameters here or through C{conf}.\n",
      " |      \n",
      " |      :param master: Cluster URL to connect to\n",
      " |             (e.g. mesos://host:port, spark://host:port, local[4]).\n",
      " |      :param appName: A name for your job, to display on the cluster web UI.\n",
      " |      :param sparkHome: Location where Spark is installed on cluster nodes.\n",
      " |      :param pyFiles: Collection of .zip or .py files to send to the cluster\n",
      " |             and add to PYTHONPATH.  These can be paths on the local file\n",
      " |             system or HDFS, HTTP, HTTPS, or FTP URLs.\n",
      " |      :param environment: A dictionary of environment variables to set on\n",
      " |             worker nodes.\n",
      " |      :param batchSize: The number of Python objects represented as a single\n",
      " |             Java object. Set 1 to disable batching, 0 to automatically choose\n",
      " |             the batch size based on object sizes, or -1 to use an unlimited\n",
      " |             batch size\n",
      " |      :param serializer: The serializer for RDDs.\n",
      " |      :param conf: A L{SparkConf} object setting Spark properties.\n",
      " |      :param gateway: Use an existing gateway and JVM, otherwise a new JVM\n",
      " |             will be instantiated.\n",
      " |      :param jsc: The JavaSparkContext instance (optional).\n",
      " |      :param profiler_cls: A class of custom Profiler used to do profiling\n",
      " |             (default is pyspark.profiler.BasicProfiler).\n",
      " |      \n",
      " |      \n",
      " |      >>> from pyspark.context import SparkContext\n",
      " |      >>> sc = SparkContext('local', 'test')\n",
      " |      \n",
      " |      >>> sc2 = SparkContext('local', 'test2') # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      " |      Traceback (most recent call last):\n",
      " |          ...\n",
      " |      ValueError:...\n",
      " |  \n",
      " |  accumulator(self, value, accum_param=None)\n",
      " |      Create an L{Accumulator} with the given initial value, using a given\n",
      " |      L{AccumulatorParam} helper object to define how to add values of the\n",
      " |      data type if provided. Default AccumulatorParams are used for integers\n",
      " |      and floating-point numbers if you do not provide one. For other types,\n",
      " |      a custom AccumulatorParam can be used.\n",
      " |  \n",
      " |  addFile(self, path, recursive=False)\n",
      " |      Add a file to be downloaded with this Spark job on every node.\n",
      " |      The C{path} passed can be either a local file, a file in HDFS\n",
      " |      (or other Hadoop-supported filesystems), or an HTTP, HTTPS or\n",
      " |      FTP URI.\n",
      " |      \n",
      " |      To access the file in Spark jobs, use\n",
      " |      L{SparkFiles.get(fileName)<pyspark.files.SparkFiles.get>} with the\n",
      " |      filename to find its download location.\n",
      " |      \n",
      " |      A directory can be given if the recursive option is set to True.\n",
      " |      Currently directories are only supported for Hadoop-supported filesystems.\n",
      " |      \n",
      " |      >>> from pyspark import SparkFiles\n",
      " |      >>> path = os.path.join(tempdir, \"test.txt\")\n",
      " |      >>> with open(path, \"w\") as testFile:\n",
      " |      ...    _ = testFile.write(\"100\")\n",
      " |      >>> sc.addFile(path)\n",
      " |      >>> def func(iterator):\n",
      " |      ...    with open(SparkFiles.get(\"test.txt\")) as testFile:\n",
      " |      ...        fileVal = int(testFile.readline())\n",
      " |      ...        return [x * fileVal for x in iterator]\n",
      " |      >>> sc.parallelize([1, 2, 3, 4]).mapPartitions(func).collect()\n",
      " |      [100, 200, 300, 400]\n",
      " |  \n",
      " |  addPyFile(self, path)\n",
      " |      Add a .py or .zip dependency for all tasks to be executed on this\n",
      " |      SparkContext in the future.  The C{path} passed can be either a local\n",
      " |      file, a file in HDFS (or other Hadoop-supported filesystems), or an\n",
      " |      HTTP, HTTPS or FTP URI.\n",
      " |  \n",
      " |  binaryFiles(self, path, minPartitions=None)\n",
      " |      .. note:: Experimental\n",
      " |      \n",
      " |      Read a directory of binary files from HDFS, a local file system\n",
      " |      (available on all nodes), or any Hadoop-supported file system URI\n",
      " |      as a byte array. Each file is read as a single record and returned\n",
      " |      in a key-value pair, where the key is the path of each file, the\n",
      " |      value is the content of each file.\n",
      " |      \n",
      " |      .. note:: Small files are preferred, large file is also allowable, but\n",
      " |          may cause bad performance.\n",
      " |  \n",
      " |  binaryRecords(self, path, recordLength)\n",
      " |      .. note:: Experimental\n",
      " |      \n",
      " |      Load data from a flat binary file, assuming each record is a set of numbers\n",
      " |      with the specified numerical format (see ByteBuffer), and the number of\n",
      " |      bytes per record is constant.\n",
      " |      \n",
      " |      :param path: Directory to the input data files\n",
      " |      :param recordLength: The length at which to split the records\n",
      " |  \n",
      " |  broadcast(self, value)\n",
      " |      Broadcast a read-only variable to the cluster, returning a\n",
      " |      L{Broadcast<pyspark.broadcast.Broadcast>}\n",
      " |      object for reading it in distributed functions. The variable will\n",
      " |      be sent to each cluster only once.\n",
      " |  \n",
      " |  cancelAllJobs(self)\n",
      " |      Cancel all jobs that have been scheduled or are running.\n",
      " |  \n",
      " |  cancelJobGroup(self, groupId)\n",
      " |      Cancel active jobs for the specified group. See L{SparkContext.setJobGroup}\n",
      " |      for more information.\n",
      " |  \n",
      " |  dump_profiles(self, path)\n",
      " |      Dump the profile stats into directory `path`\n",
      " |  \n",
      " |  emptyRDD(self)\n",
      " |      Create an RDD that has no partitions or elements.\n",
      " |  \n",
      " |  getConf(self)\n",
      " |  \n",
      " |  getLocalProperty(self, key)\n",
      " |      Get a local property set in this thread, or null if it is missing. See\n",
      " |      L{setLocalProperty}\n",
      " |  \n",
      " |  hadoopFile(self, path, inputFormatClass, keyClass, valueClass, keyConverter=None, valueConverter=None, conf=None, batchSize=0)\n",
      " |      Read an 'old' Hadoop InputFormat with arbitrary key and value class from HDFS,\n",
      " |      a local file system (available on all nodes), or any Hadoop-supported file system URI.\n",
      " |      The mechanism is the same as for sc.sequenceFile.\n",
      " |      \n",
      " |      A Hadoop configuration can be passed in as a Python dict. This will be converted into a\n",
      " |      Configuration in Java.\n",
      " |      \n",
      " |      :param path: path to Hadoop file\n",
      " |      :param inputFormatClass: fully qualified classname of Hadoop InputFormat\n",
      " |             (e.g. \"org.apache.hadoop.mapred.TextInputFormat\")\n",
      " |      :param keyClass: fully qualified classname of key Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.Text\")\n",
      " |      :param valueClass: fully qualified classname of value Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.LongWritable\")\n",
      " |      :param keyConverter: (None by default)\n",
      " |      :param valueConverter: (None by default)\n",
      " |      :param conf: Hadoop configuration, passed in as a dict\n",
      " |             (None by default)\n",
      " |      :param batchSize: The number of Python objects represented as a single\n",
      " |             Java object. (default 0, choose batchSize automatically)\n",
      " |  \n",
      " |  hadoopRDD(self, inputFormatClass, keyClass, valueClass, keyConverter=None, valueConverter=None, conf=None, batchSize=0)\n",
      " |      Read an 'old' Hadoop InputFormat with arbitrary key and value class, from an arbitrary\n",
      " |      Hadoop configuration, which is passed in as a Python dict.\n",
      " |      This will be converted into a Configuration in Java.\n",
      " |      The mechanism is the same as for sc.sequenceFile.\n",
      " |      \n",
      " |      :param inputFormatClass: fully qualified classname of Hadoop InputFormat\n",
      " |             (e.g. \"org.apache.hadoop.mapred.TextInputFormat\")\n",
      " |      :param keyClass: fully qualified classname of key Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.Text\")\n",
      " |      :param valueClass: fully qualified classname of value Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.LongWritable\")\n",
      " |      :param keyConverter: (None by default)\n",
      " |      :param valueConverter: (None by default)\n",
      " |      :param conf: Hadoop configuration, passed in as a dict\n",
      " |             (None by default)\n",
      " |      :param batchSize: The number of Python objects represented as a single\n",
      " |             Java object. (default 0, choose batchSize automatically)\n",
      " |  \n",
      " |  newAPIHadoopFile(self, path, inputFormatClass, keyClass, valueClass, keyConverter=None, valueConverter=None, conf=None, batchSize=0)\n",
      " |      Read a 'new API' Hadoop InputFormat with arbitrary key and value class from HDFS,\n",
      " |      a local file system (available on all nodes), or any Hadoop-supported file system URI.\n",
      " |      The mechanism is the same as for sc.sequenceFile.\n",
      " |      \n",
      " |      A Hadoop configuration can be passed in as a Python dict. This will be converted into a\n",
      " |      Configuration in Java\n",
      " |      \n",
      " |      :param path: path to Hadoop file\n",
      " |      :param inputFormatClass: fully qualified classname of Hadoop InputFormat\n",
      " |             (e.g. \"org.apache.hadoop.mapreduce.lib.input.TextInputFormat\")\n",
      " |      :param keyClass: fully qualified classname of key Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.Text\")\n",
      " |      :param valueClass: fully qualified classname of value Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.LongWritable\")\n",
      " |      :param keyConverter: (None by default)\n",
      " |      :param valueConverter: (None by default)\n",
      " |      :param conf: Hadoop configuration, passed in as a dict\n",
      " |             (None by default)\n",
      " |      :param batchSize: The number of Python objects represented as a single\n",
      " |             Java object. (default 0, choose batchSize automatically)\n",
      " |  \n",
      " |  newAPIHadoopRDD(self, inputFormatClass, keyClass, valueClass, keyConverter=None, valueConverter=None, conf=None, batchSize=0)\n",
      " |      Read a 'new API' Hadoop InputFormat with arbitrary key and value class, from an arbitrary\n",
      " |      Hadoop configuration, which is passed in as a Python dict.\n",
      " |      This will be converted into a Configuration in Java.\n",
      " |      The mechanism is the same as for sc.sequenceFile.\n",
      " |      \n",
      " |      :param inputFormatClass: fully qualified classname of Hadoop InputFormat\n",
      " |             (e.g. \"org.apache.hadoop.mapreduce.lib.input.TextInputFormat\")\n",
      " |      :param keyClass: fully qualified classname of key Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.Text\")\n",
      " |      :param valueClass: fully qualified classname of value Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.LongWritable\")\n",
      " |      :param keyConverter: (None by default)\n",
      " |      :param valueConverter: (None by default)\n",
      " |      :param conf: Hadoop configuration, passed in as a dict\n",
      " |             (None by default)\n",
      " |      :param batchSize: The number of Python objects represented as a single\n",
      " |             Java object. (default 0, choose batchSize automatically)\n",
      " |  \n",
      " |  parallelize(self, c, numSlices=None)\n",
      " |      Distribute a local Python collection to form an RDD. Using xrange\n",
      " |      is recommended if the input represents a range for performance.\n",
      " |      \n",
      " |      >>> sc.parallelize([0, 2, 3, 4, 6], 5).glom().collect()\n",
      " |      [[0], [2], [3], [4], [6]]\n",
      " |      >>> sc.parallelize(xrange(0, 6, 2), 5).glom().collect()\n",
      " |      [[], [0], [], [2], [4]]\n",
      " |  \n",
      " |  pickleFile(self, name, minPartitions=None)\n",
      " |      Load an RDD previously saved using L{RDD.saveAsPickleFile} method.\n",
      " |      \n",
      " |      >>> tmpFile = NamedTemporaryFile(delete=True)\n",
      " |      >>> tmpFile.close()\n",
      " |      >>> sc.parallelize(range(10)).saveAsPickleFile(tmpFile.name, 5)\n",
      " |      >>> sorted(sc.pickleFile(tmpFile.name, 3).collect())\n",
      " |      [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      " |  \n",
      " |  range(self, start, end=None, step=1, numSlices=None)\n",
      " |      Create a new RDD of int containing elements from `start` to `end`\n",
      " |      (exclusive), increased by `step` every element. Can be called the same\n",
      " |      way as python's built-in range() function. If called with a single argument,\n",
      " |      the argument is interpreted as `end`, and `start` is set to 0.\n",
      " |      \n",
      " |      :param start: the start value\n",
      " |      :param end: the end value (exclusive)\n",
      " |      :param step: the incremental step (default: 1)\n",
      " |      :param numSlices: the number of partitions of the new RDD\n",
      " |      :return: An RDD of int\n",
      " |      \n",
      " |      >>> sc.range(5).collect()\n",
      " |      [0, 1, 2, 3, 4]\n",
      " |      >>> sc.range(2, 4).collect()\n",
      " |      [2, 3]\n",
      " |      >>> sc.range(1, 7, 2).collect()\n",
      " |      [1, 3, 5]\n",
      " |  \n",
      " |  runJob(self, rdd, partitionFunc, partitions=None, allowLocal=False)\n",
      " |      Executes the given partitionFunc on the specified set of partitions,\n",
      " |      returning the result as an array of elements.\n",
      " |      \n",
      " |      If 'partitions' is not specified, this will run over all partitions.\n",
      " |      \n",
      " |      >>> myRDD = sc.parallelize(range(6), 3)\n",
      " |      >>> sc.runJob(myRDD, lambda part: [x * x for x in part])\n",
      " |      [0, 1, 4, 9, 16, 25]\n",
      " |      \n",
      " |      >>> myRDD = sc.parallelize(range(6), 3)\n",
      " |      >>> sc.runJob(myRDD, lambda part: [x * x for x in part], [0, 2], True)\n",
      " |      [0, 1, 16, 25]\n",
      " |  \n",
      " |  sequenceFile(self, path, keyClass=None, valueClass=None, keyConverter=None, valueConverter=None, minSplits=None, batchSize=0)\n",
      " |      Read a Hadoop SequenceFile with arbitrary key and value Writable class from HDFS,\n",
      " |      a local file system (available on all nodes), or any Hadoop-supported file system URI.\n",
      " |      The mechanism is as follows:\n",
      " |      \n",
      " |          1. A Java RDD is created from the SequenceFile or other InputFormat, and the key\n",
      " |             and value Writable classes\n",
      " |          2. Serialization is attempted via Pyrolite pickling\n",
      " |          3. If this fails, the fallback is to call 'toString' on each key and value\n",
      " |          4. C{PickleSerializer} is used to deserialize pickled objects on the Python side\n",
      " |      \n",
      " |      :param path: path to sequncefile\n",
      " |      :param keyClass: fully qualified classname of key Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.Text\")\n",
      " |      :param valueClass: fully qualified classname of value Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.LongWritable\")\n",
      " |      :param keyConverter:\n",
      " |      :param valueConverter:\n",
      " |      :param minSplits: minimum splits in dataset\n",
      " |             (default min(2, sc.defaultParallelism))\n",
      " |      :param batchSize: The number of Python objects represented as a single\n",
      " |             Java object. (default 0, choose batchSize automatically)\n",
      " |  \n",
      " |  setCheckpointDir(self, dirName)\n",
      " |      Set the directory under which RDDs are going to be checkpointed. The\n",
      " |      directory must be a HDFS path if running on a cluster.\n",
      " |  \n",
      " |  setJobGroup(self, groupId, description, interruptOnCancel=False)\n",
      " |      Assigns a group ID to all the jobs started by this thread until the group ID is set to a\n",
      " |      different value or cleared.\n",
      " |      \n",
      " |      Often, a unit of execution in an application consists of multiple Spark actions or jobs.\n",
      " |      Application programmers can use this method to group all those jobs together and give a\n",
      " |      group description. Once set, the Spark web UI will associate such jobs with this group.\n",
      " |      \n",
      " |      The application can use L{SparkContext.cancelJobGroup} to cancel all\n",
      " |      running jobs in this group.\n",
      " |      \n",
      " |      >>> import threading\n",
      " |      >>> from time import sleep\n",
      " |      >>> result = \"Not Set\"\n",
      " |      >>> lock = threading.Lock()\n",
      " |      >>> def map_func(x):\n",
      " |      ...     sleep(100)\n",
      " |      ...     raise Exception(\"Task should have been cancelled\")\n",
      " |      >>> def start_job(x):\n",
      " |      ...     global result\n",
      " |      ...     try:\n",
      " |      ...         sc.setJobGroup(\"job_to_cancel\", \"some description\")\n",
      " |      ...         result = sc.parallelize(range(x)).map(map_func).collect()\n",
      " |      ...     except Exception as e:\n",
      " |      ...         result = \"Cancelled\"\n",
      " |      ...     lock.release()\n",
      " |      >>> def stop_job():\n",
      " |      ...     sleep(5)\n",
      " |      ...     sc.cancelJobGroup(\"job_to_cancel\")\n",
      " |      >>> supress = lock.acquire()\n",
      " |      >>> supress = threading.Thread(target=start_job, args=(10,)).start()\n",
      " |      >>> supress = threading.Thread(target=stop_job).start()\n",
      " |      >>> supress = lock.acquire()\n",
      " |      >>> print(result)\n",
      " |      Cancelled\n",
      " |      \n",
      " |      If interruptOnCancel is set to true for the job group, then job cancellation will result\n",
      " |      in Thread.interrupt() being called on the job's executor threads. This is useful to help\n",
      " |      ensure that the tasks are actually stopped in a timely manner, but is off by default due\n",
      " |      to HDFS-1208, where HDFS may respond to Thread.interrupt() by marking nodes as dead.\n",
      " |  \n",
      " |  setLocalProperty(self, key, value)\n",
      " |      Set a local property that affects jobs submitted from this thread, such as the\n",
      " |      Spark fair scheduler pool.\n",
      " |  \n",
      " |  setLogLevel(self, logLevel)\n",
      " |      Control our logLevel. This overrides any user-defined log settings.\n",
      " |      Valid log levels include: ALL, DEBUG, ERROR, FATAL, INFO, OFF, TRACE, WARN\n",
      " |  \n",
      " |  show_profiles(self)\n",
      " |      Print the profile stats to stdout\n",
      " |  \n",
      " |  sparkUser(self)\n",
      " |      Get SPARK_USER for user who is running SparkContext.\n",
      " |  \n",
      " |  statusTracker(self)\n",
      " |      Return :class:`StatusTracker` object\n",
      " |  \n",
      " |  stop(self)\n",
      " |      Shut down the SparkContext.\n",
      " |  \n",
      " |  textFile(self, name, minPartitions=None, use_unicode=True)\n",
      " |      Read a text file from HDFS, a local file system (available on all\n",
      " |      nodes), or any Hadoop-supported file system URI, and return it as an\n",
      " |      RDD of Strings.\n",
      " |      \n",
      " |      If use_unicode is False, the strings will be kept as `str` (encoding\n",
      " |      as `utf-8`), which is faster and smaller than unicode. (Added in\n",
      " |      Spark 1.2)\n",
      " |      \n",
      " |      >>> path = os.path.join(tempdir, \"sample-text.txt\")\n",
      " |      >>> with open(path, \"w\") as testFile:\n",
      " |      ...    _ = testFile.write(\"Hello world!\")\n",
      " |      >>> textFile = sc.textFile(path)\n",
      " |      >>> textFile.collect()\n",
      " |      [u'Hello world!']\n",
      " |  \n",
      " |  union(self, rdds)\n",
      " |      Build the union of a list of RDDs.\n",
      " |      \n",
      " |      This supports unions() of RDDs with different serialized formats,\n",
      " |      although this forces them to be reserialized using the default\n",
      " |      serializer:\n",
      " |      \n",
      " |      >>> path = os.path.join(tempdir, \"union-text.txt\")\n",
      " |      >>> with open(path, \"w\") as testFile:\n",
      " |      ...    _ = testFile.write(\"Hello\")\n",
      " |      >>> textFile = sc.textFile(path)\n",
      " |      >>> textFile.collect()\n",
      " |      [u'Hello']\n",
      " |      >>> parallelized = sc.parallelize([\"World!\"])\n",
      " |      >>> sorted(sc.union([textFile, parallelized]).collect())\n",
      " |      [u'Hello', 'World!']\n",
      " |  \n",
      " |  wholeTextFiles(self, path, minPartitions=None, use_unicode=True)\n",
      " |      Read a directory of text files from HDFS, a local file system\n",
      " |      (available on all nodes), or any  Hadoop-supported file system\n",
      " |      URI. Each file is read as a single record and returned in a\n",
      " |      key-value pair, where the key is the path of each file, the\n",
      " |      value is the content of each file.\n",
      " |      \n",
      " |      If use_unicode is False, the strings will be kept as `str` (encoding\n",
      " |      as `utf-8`), which is faster and smaller than unicode. (Added in\n",
      " |      Spark 1.2)\n",
      " |      \n",
      " |      For example, if you have the following files::\n",
      " |      \n",
      " |        hdfs://a-hdfs-path/part-00000\n",
      " |        hdfs://a-hdfs-path/part-00001\n",
      " |        ...\n",
      " |        hdfs://a-hdfs-path/part-nnnnn\n",
      " |      \n",
      " |      Do C{rdd = sparkContext.wholeTextFiles(\"hdfs://a-hdfs-path\")},\n",
      " |      then C{rdd} contains::\n",
      " |      \n",
      " |        (a-hdfs-path/part-00000, its content)\n",
      " |        (a-hdfs-path/part-00001, its content)\n",
      " |        ...\n",
      " |        (a-hdfs-path/part-nnnnn, its content)\n",
      " |      \n",
      " |      .. note:: Small files are preferred, as each file will be loaded\n",
      " |          fully in memory.\n",
      " |      \n",
      " |      >>> dirPath = os.path.join(tempdir, \"files\")\n",
      " |      >>> os.mkdir(dirPath)\n",
      " |      >>> with open(os.path.join(dirPath, \"1.txt\"), \"w\") as file1:\n",
      " |      ...    _ = file1.write(\"1\")\n",
      " |      >>> with open(os.path.join(dirPath, \"2.txt\"), \"w\") as file2:\n",
      " |      ...    _ = file2.write(\"2\")\n",
      " |      >>> textFiles = sc.wholeTextFiles(dirPath)\n",
      " |      >>> sorted(textFiles.collect())\n",
      " |      [(u'.../1.txt', u'1'), (u'.../2.txt', u'2')]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  getOrCreate(cls, conf=None) from __builtin__.type\n",
      " |      Get or instantiate a SparkContext and register it as a singleton object.\n",
      " |      \n",
      " |      :param conf: SparkConf (optional)\n",
      " |  \n",
      " |  setSystemProperty(cls, key, value) from __builtin__.type\n",
      " |      Set a Java system property, such as spark.executor.memory. This must\n",
      " |      must be invoked before instantiating SparkContext.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  applicationId\n",
      " |      A unique identifier for the Spark application.\n",
      " |      Its format depends on the scheduler implementation.\n",
      " |      \n",
      " |      * in case of local spark app something like 'local-1433865536131'\n",
      " |      * in case of YARN something like 'application_1433865536131_34483'\n",
      " |      \n",
      " |      >>> sc.applicationId  # doctest: +ELLIPSIS\n",
      " |      u'local-...'\n",
      " |  \n",
      " |  defaultMinPartitions\n",
      " |      Default min number of partitions for Hadoop RDDs when not given by user\n",
      " |  \n",
      " |  defaultParallelism\n",
      " |      Default level of parallelism to use when not given by user (e.g. for\n",
      " |      reduce tasks)\n",
      " |  \n",
      " |  startTime\n",
      " |      Return the epoch time when the Spark Context was started.\n",
      " |  \n",
      " |  uiWebUrl\n",
      " |      Return the URL of the SparkUI instance started by this SparkContext\n",
      " |  \n",
      " |  version\n",
      " |      The version of Spark on which this application is running.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  PACKAGE_EXTENSIONS = ('.zip', '.egg', '.jar')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'2.1.1'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After reading the help we've decided we want to use sc.version to see what version of Spark we are running\n",
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = xrange(1, 10001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can check the size of the list using the len() function\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = xrange(1, 1000000)\n",
    "xrangeRDD = sc.parallelize(data, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see how many partitions the RDD will be split into by using the getNumPartitions()\n",
    "xrangeRDD.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![partitions](http://spark-mooc.github.io/web-assets/images/partitions.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sub(value):\n",
    "    \"\"\"\"Subtracts one from `value`.\n",
    "\n",
    "    Args:\n",
    "       value (int): A number.\n",
    "\n",
    "    Returns:\n",
    "        int: `value` minus one.\n",
    "    \"\"\"\n",
    "    return (value - 1)\n",
    "\n",
    "subRDD = xrangeRDD.map(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999999\n"
     ]
    }
   ],
   "source": [
    "print subRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subRDD.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999999\n"
     ]
    }
   ],
   "source": [
    "print xrangeRDD.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![filter](http://spark-mooc.github.io/web-assets/images/filter.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "def ten(value):\n",
    "    \"\"\"Return whether value is below ten.\n",
    "\n",
    "    Args:\n",
    "        value (int): A number.\n",
    "\n",
    "    Returns:\n",
    "        bool: Whether `value` is less than ten.\n",
    "    \"\"\"\n",
    "    if (value < 10):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "filteredRDD = subRDD.filter(ten)\n",
    "\n",
    "print filteredRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambdaRDD = subRDD.filter(lambda x: x < 10)\n",
    "lambdaRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 4, 6, 8]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evenRDD = lambdaRDD.filter(lambda x: x % 2 == 0)\n",
    "evenRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Additional RDD actions **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[0, 1, 2, 3]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "print filteredRDD.first()\n",
    "\n",
    "print filteredRDD.take(4)\n",
    "\n",
    "print filteredRDD.take(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2]\n",
      "[9, 8, 7, 6, 5]\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the three smallest elements\n",
    "print filteredRDD.takeOrdered(3)\n",
    "# Retrieve the five largest elements\n",
    "print filteredRDD.top(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 8, 7, 6]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filteredRDD.takeOrdered(4, lambda s: -s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "from operator import add\n",
    "\n",
    "print filteredRDD.reduce(add)\n",
    "\n",
    "print filteredRDD.reduce(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 0, 4, 1, 7]\n",
      "[7, 3, 4, 2, 5, 0]\n"
     ]
    }
   ],
   "source": [
    "print filteredRDD.takeSample(withReplacement=True, num=6)\n",
    "\n",
    "print filteredRDD.takeSample(withReplacement=False, num=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 5, 3, 6, 9]\n"
     ]
    }
   ],
   "source": [
    "print filteredRDD.takeSample(withReplacement=False, num=6, seed=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<type 'int'>, {1: 4, 2: 4, 3: 5, 4: 2, 5: 1, 6: 1})\n"
     ]
    }
   ],
   "source": [
    "repetitiveRDD = sc.parallelize([1, 2, 3, 1, 2, 3, 1, 2, 1, 2, 3, 3, 3, 4, 5, 4, 6])\n",
    "print repetitiveRDD.countByValue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Additional RDD transformations **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cat', 'cats'), ('elephant', 'elephants'), ('rat', 'rats'), ('cat', 'cats')]\n",
      "['cat', 'cats', 'elephant', 'elephants', 'rat', 'rats', 'cat', 'cats']\n",
      "4\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "wordsList = ['cat', 'elephant', 'rat', 'cat']\n",
    "wordsRDD = sc.parallelize(wordsList, 4)\n",
    "\n",
    "singularAndPluralWordsRDDMap = wordsRDD.map(lambda x: (x, x + 's'))\n",
    "singularAndPluralWordsRDD = wordsRDD.flatMap(lambda x: (x, x + 's'))\n",
    "\n",
    "print singularAndPluralWordsRDDMap.collect()\n",
    "print singularAndPluralWordsRDD.collect()\n",
    "print singularAndPluralWordsRDDMap.count()\n",
    "print singularAndPluralWordsRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1], [1, 2], [1, 2, 3]]\n",
      "[1, 1, 2, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "simpleRDD = sc.parallelize([2, 3, 4])\n",
    "print simpleRDD.map(lambda x: range(1, x)).collect()\n",
    "print simpleRDD.flatMap(lambda x: range(1, x)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **  `groupByKey` and `reduceByKey` **\n",
    "\n",
    "\n",
    "![reduceByKey() figure](http://spark-mooc.github.io/web-assets/images/reduce_by.png)\n",
    "\n",
    "![groupByKey() figure](http://spark-mooc.github.io/web-assets/images/group_by.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', [1, 2]), ('b', [1])]\n",
      "[('a', 3), ('b', 1)]\n",
      "[('a', 3), ('b', 1)]\n",
      "[('a', 3), ('b', 1)]\n"
     ]
    }
   ],
   "source": [
    "pairRDD = sc.parallelize([('a', 1), ('a', 2), ('b', 1)])\n",
    "\n",
    "print pairRDD.groupByKey().mapValues(lambda x: list(x)).collect()\n",
    "\n",
    "print pairRDD.groupByKey().map(lambda (k, v): (k, sum(v))).collect()\n",
    "print pairRDD.groupByKey().mapValues(lambda x: sum(x)).collect()\n",
    "print pairRDD.reduceByKey(add).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching RDDs and storage options "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "filteredRDD.cache()\n",
    "print filteredRDD.is_cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8) PythonRDD[3] at collect at <ipython-input-18-870118197f8d>:17 [Memory Serialized 1x Replicated]\n",
      " |  ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:475 [Memory Serialized 1x Replicated]\n"
     ]
    }
   ],
   "source": [
    "print filteredRDD.toDebugString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serialized 1x Replicated\n",
      "Memory Serialized 1x Replicated\n"
     ]
    }
   ],
   "source": [
    "filteredRDD.unpersist()\n",
    "\n",
    "print filteredRDD.getStorageLevel()\n",
    "filteredRDD.cache()\n",
    "print filteredRDD.getStorageLevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filteredRDD.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrames "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlCtx = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "employeeSchema = StructType([\n",
    "                       StructField('id', IntegerType(), nullable=False),\n",
    "                       StructField('first_name', StringType(), nullable=False),\n",
    "                       StructField('last_name', StringType(), nullable=False),\n",
    "                       StructField('email', StringType(), nullable=False),\n",
    "                       StructField('phone', StringType(), nullable=False),\n",
    "                       StructField('department', StringType(), nullable=False)\n",
    "                      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(id,IntegerType,false),StructField(first_name,StringType,false),StructField(last_name,StringType,false),StructField(email,StringType,false),StructField(phone,StringType,false),StructField(department,StringType,false)))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employeeSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "employeeData = sc.parallelize([\n",
    "                                Row(1,'John','Doe','john.doe@company.com','555-12345', 'Sales'),\n",
    "                                Row(2,'Johnny','Wishbone','johnny.wishbone@company.com','555-01534','IT'),\n",
    "                                Row(3,'Max','Mustermann','max.mustermann@company.com','555-45678','IT'),\n",
    "                                Row(4,'Helga','Musterfrau','helga.musterfrau@company.com','555-0963','Business'),\n",
    "                                Row(5,'George','Schmidt','george.schmidt@company.com','555-67232','Business'),\n",
    "                                Row(6,'Bob','Moore','bob.moore@company.com','555-78123','Sales'),\n",
    "                                Row(6,'Alex','Warny','alex.warny@company.com','555-4567','Operations'),\n",
    "                                Row(6,'Fred','Bernoulli','fred.bernoulli@company.com','555-0984','Operations'),\n",
    "                                Row(6,'Martha','Richards','martha.richards@company.com','555-45123','Acquisitions'),\n",
    "                                Row(6,'Helena','Cartier','helena.cartier@company.com','555-34908','Acquisitions')\n",
    "                             ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[21] at parallelize at PythonRDD.scala:475"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employeeData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "employeeDF = sqlCtx.createDataFrame(employeeData, employeeSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|first_name| last_name|\n",
      "+----------+----------+\n",
      "|      John|       Doe|\n",
      "|    Johnny|  Wishbone|\n",
      "|       Max|Mustermann|\n",
      "|     Helga|Musterfrau|\n",
      "|    George|   Schmidt|\n",
      "|       Bob|     Moore|\n",
      "|      Alex|     Warny|\n",
      "|      Fred| Bernoulli|\n",
      "|    Martha|  Richards|\n",
      "|    Helena|   Cartier|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selected = employeeDF.select('first_name','last_name')\n",
    "selected.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(email=u'john.doe@company.com'), Row(email=u'johnny.wishbone@company.com')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employeeDF.select('email').take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|       group|\n",
      "+------------+\n",
      "|       Sales|\n",
      "|          IT|\n",
      "|          IT|\n",
      "|    Business|\n",
      "|    Business|\n",
      "|       Sales|\n",
      "|  Operations|\n",
      "|  Operations|\n",
      "|Acquisitions|\n",
      "|Acquisitions|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as func\n",
    "\n",
    "employeeDF.select(func.col('department').alias('group')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               email|count|\n",
      "+--------------------+-----+\n",
      "|helga.musterfrau@...|    1|\n",
      "|george.schmidt@co...|    1|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeeDF.filter(func.col('department') == 'Business').groupBy('email').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yet another example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "comments_json = './results-20151009-103553.json' \n",
    "with open(comments_json) as cmnts:              \n",
    "    all_entries = cmnts.readlines()             \n",
    "\n",
    "parsable_data = \"[\" + ','.join(all_entries).replace('\\n','') + \"]\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "comments = pd.read_json(parsable_data, orient='columns',typ='frame',convert_dates=['created'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sparkDF = sqlCtx.createDataFrame(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- author: string (nullable = true)\n",
      " |-- avg_score: long (nullable = true)\n",
      " |-- comment: string (nullable = true)\n",
      " |-- controversiality: long (nullable = true)\n",
      " |-- created: long (nullable = true)\n",
      " |-- distinguished: string (nullable = true)\n",
      " |-- downs: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- subr: string (nullable = true)\n",
      " |-- ups: long (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sparkDF.write.parquet(\"./results-20151009-103553.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parquet = sqlCtx.read.parquet(\"./results-20151009-103553.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pandas_df = parquet.where(func.col('avg_score') > 10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>avg_score</th>\n",
       "      <th>comment</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>created</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>downs</th>\n",
       "      <th>name</th>\n",
       "      <th>subr</th>\n",
       "      <th>ups</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Keundrum</td>\n",
       "      <td>3877</td>\n",
       "      <td>I don't know why they would possibly tell peop...</td>\n",
       "      <td>0</td>\n",
       "      <td>1439958069000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>t1_cu7x0ut</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>3877</td>\n",
       "      <td>http://reddit.com/r/worldnews/comments/3hidw9/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>McBjure</td>\n",
       "      <td>13</td>\n",
       "      <td>He wont earn shit because it's against youtube...</td>\n",
       "      <td>0</td>\n",
       "      <td>1439230955000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>t1_cty7slu</td>\n",
       "      <td>videos</td>\n",
       "      <td>13</td>\n",
       "      <td>http://reddit.com/r/videos/comments/3ggt2v/c/c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Egsession</td>\n",
       "      <td>37</td>\n",
       "      <td>They weren't dating that long though -- not ev...</td>\n",
       "      <td>0</td>\n",
       "      <td>1438453918000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>t1_cto3vte</td>\n",
       "      <td>movies</td>\n",
       "      <td>37</td>\n",
       "      <td>http://reddit.com/r/movies/comments/3fe8ru/c/c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fresh_prince_</td>\n",
       "      <td>13</td>\n",
       "      <td>kek</td>\n",
       "      <td>0</td>\n",
       "      <td>1439654076000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>t1_cu3ync9</td>\n",
       "      <td>DotA2</td>\n",
       "      <td>13</td>\n",
       "      <td>http://reddit.com/r/DotA2/comments/3h30x7/c/cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>djohn_14</td>\n",
       "      <td>11</td>\n",
       "      <td>I think DS2's DLC had AMAZING music, especiall...</td>\n",
       "      <td>0</td>\n",
       "      <td>1438713706000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>t1_ctrdvzs</td>\n",
       "      <td>Games</td>\n",
       "      <td>11</td>\n",
       "      <td>http://reddit.com/r/Games/comments/3fr6dt/c/ct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MissyPie</td>\n",
       "      <td>30</td>\n",
       "      <td>Thank you! ^ ^\\n\\nI'm sorry, my heart belongs ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1440596771000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>t1_cug8byb</td>\n",
       "      <td>anime</td>\n",
       "      <td>30</td>\n",
       "      <td>http://reddit.com/r/anime/comments/3igs0f/c/cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Nilwx</td>\n",
       "      <td>23</td>\n",
       "      <td>\"Spaceship\"</td>\n",
       "      <td>0</td>\n",
       "      <td>1439068042000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>t1_ctw8zz5</td>\n",
       "      <td>GlobalOffensive</td>\n",
       "      <td>23</td>\n",
       "      <td>http://reddit.com/r/GlobalOffensive/comments/3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Mike_Gainer</td>\n",
       "      <td>78</td>\n",
       "      <td>Mastodon: Pendulous Skin\\nhttps://www.youtube....</td>\n",
       "      <td>0</td>\n",
       "      <td>1440167085000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>t1_cuao760</td>\n",
       "      <td>Music</td>\n",
       "      <td>78</td>\n",
       "      <td>http://reddit.com/r/Music/comments/3hu1mf/c/cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>nopooq</td>\n",
       "      <td>11</td>\n",
       "      <td>Yes! This happens SO often. Thank you for shar...</td>\n",
       "      <td>0</td>\n",
       "      <td>1440174263000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>t1_cuaswna</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>11</td>\n",
       "      <td>http://reddit.com/r/AskReddit/comments/3hu28g/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LutherDingle</td>\n",
       "      <td>11</td>\n",
       "      <td>The article characterizes Trump as an outsider...</td>\n",
       "      <td>0</td>\n",
       "      <td>1438699354000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>t1_ctr482t</td>\n",
       "      <td>politics</td>\n",
       "      <td>11</td>\n",
       "      <td>http://reddit.com/r/politics/comments/3fqr96/c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>inuyome1008</td>\n",
       "      <td>94</td>\n",
       "      <td>JESUS WEPT!</td>\n",
       "      <td>0</td>\n",
       "      <td>1440736851000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>t1_cuibege</td>\n",
       "      <td>gaming</td>\n",
       "      <td>94</td>\n",
       "      <td>http://reddit.com/r/gaming/comments/3io172/c/c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>cyborgcommando0</td>\n",
       "      <td>45</td>\n",
       "      <td>Halo 1 pistol.</td>\n",
       "      <td>0</td>\n",
       "      <td>1439483902000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>t1_cu1qceu</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>45</td>\n",
       "      <td>http://reddit.com/r/AskReddit/comments/3gv455/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Ekman-ish</td>\n",
       "      <td>26</td>\n",
       "      <td>Not a diver, but I'm thinking you wouldn't act...</td>\n",
       "      <td>0</td>\n",
       "      <td>1440968757000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>t1_cul4aay</td>\n",
       "      <td>askscience</td>\n",
       "      <td>26</td>\n",
       "      <td>http://reddit.com/r/askscience/comments/3iykdi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>throwmeaway123456780</td>\n",
       "      <td>30</td>\n",
       "      <td>The white community needs to step up and voice...</td>\n",
       "      <td>0</td>\n",
       "      <td>1440968967000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>t1_cul4ehb</td>\n",
       "      <td>news</td>\n",
       "      <td>30</td>\n",
       "      <td>http://reddit.com/r/news/comments/3iyrqb/c/cul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>clone9786</td>\n",
       "      <td>47</td>\n",
       "      <td>Why do people hate this so much? It's definite...</td>\n",
       "      <td>0</td>\n",
       "      <td>1439252732000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>t1_ctylkk1</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>47</td>\n",
       "      <td>http://reddit.com/r/AskReddit/comments/3gi7dv/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Holy_Kamikaze_Batman</td>\n",
       "      <td>98</td>\n",
       "      <td>http://i.imgur.com/rB4Hux7.gif</td>\n",
       "      <td>0</td>\n",
       "      <td>1438635380000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>t1_ctqbxrl</td>\n",
       "      <td>pics</td>\n",
       "      <td>98</td>\n",
       "      <td>http://reddit.com/r/pics/comments/3fnb7p/c/ctq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>uprons</td>\n",
       "      <td>16</td>\n",
       "      <td>Any idea if Korean will be added?</td>\n",
       "      <td>0</td>\n",
       "      <td>1438511347000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>t1_ctorarl</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>16</td>\n",
       "      <td>http://reddit.com/r/AskReddit/comments/3fgxoz/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ScousePie</td>\n",
       "      <td>17</td>\n",
       "      <td>That was Chelsea, after we sent them a polite ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1439919074000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>t1_cu792lo</td>\n",
       "      <td>soccer</td>\n",
       "      <td>17</td>\n",
       "      <td>http://reddit.com/r/soccer/comments/3hgs01/c/c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>RedOrmTostesson</td>\n",
       "      <td>106</td>\n",
       "      <td>I actually liked his set. It was pretty fun if...</td>\n",
       "      <td>0</td>\n",
       "      <td>1439166604000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>t1_ctxfay0</td>\n",
       "      <td>DotA2</td>\n",
       "      <td>106</td>\n",
       "      <td>http://reddit.com/r/DotA2/comments/3geei8/c/ct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>CameraMan1</td>\n",
       "      <td>17</td>\n",
       "      <td>at yeast you tried...</td>\n",
       "      <td>0</td>\n",
       "      <td>1438408661000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>t1_ctnp1sc</td>\n",
       "      <td>woahdude</td>\n",
       "      <td>17</td>\n",
       "      <td>http://reddit.com/r/woahdude/comments/3fc09i/c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>dr3blira</td>\n",
       "      <td>26</td>\n",
       "      <td>There's a huge pottery fair once a year near m...</td>\n",
       "      <td>0</td>\n",
       "      <td>1440357891000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>t1_cud2po0</td>\n",
       "      <td>TrollXChromosomes</td>\n",
       "      <td>26</td>\n",
       "      <td>http://reddit.com/r/TrollXChromosomes/comments...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>TKDbeast</td>\n",
       "      <td>184</td>\n",
       "      <td>Astute observation, /u/Israeli_Shill.</td>\n",
       "      <td>0</td>\n",
       "      <td>1438629057000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>t1_ctq7j45</td>\n",
       "      <td>pokemon</td>\n",
       "      <td>184</td>\n",
       "      <td>http://reddit.com/r/pokemon/comments/3fn2yj/c/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>BunniesWithRabies</td>\n",
       "      <td>17</td>\n",
       "      <td>By design *and* choice here\\n\\nI find it reall...</td>\n",
       "      <td>0</td>\n",
       "      <td>1439072227000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>t1_ctwaayd</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>17</td>\n",
       "      <td>http://reddit.com/r/AskReddit/comments/3g8epl/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>chocorazor</td>\n",
       "      <td>16</td>\n",
       "      <td>Much more impressive looking at live speed.</td>\n",
       "      <td>0</td>\n",
       "      <td>1439059232000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>t1_ctw4j40</td>\n",
       "      <td>soccer</td>\n",
       "      <td>16</td>\n",
       "      <td>http://reddit.com/r/soccer/comments/3g92pd/c/c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Princepurple1</td>\n",
       "      <td>14</td>\n",
       "      <td>1. Using the Bear as a Puffin. 2. This is in n...</td>\n",
       "      <td>0</td>\n",
       "      <td>1439230726000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>t1_cty7n02</td>\n",
       "      <td>AdviceAnimals</td>\n",
       "      <td>14</td>\n",
       "      <td>http://reddit.com/r/AdviceAnimals/comments/3gg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>dontfuckingdennisme</td>\n",
       "      <td>259</td>\n",
       "      <td>So just drive across the border. Zambia, in Ma...</td>\n",
       "      <td>0</td>\n",
       "      <td>1438661150000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>t1_ctqqxoc</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>259</td>\n",
       "      <td>http://reddit.com/r/worldnews/comments/3foob1/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>NecroGod</td>\n",
       "      <td>95</td>\n",
       "      <td>&amp;gt; and start cleaning things.\\n\\nWell, take ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1438873990000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>t1_cttmisy</td>\n",
       "      <td>WTF</td>\n",
       "      <td>95</td>\n",
       "      <td>http://reddit.com/r/WTF/comments/3fzv89/c/cttmisy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>choixpeau</td>\n",
       "      <td>26</td>\n",
       "      <td>I wonder why they thought a military school wo...</td>\n",
       "      <td>0</td>\n",
       "      <td>1439620972000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>t1_cu3p1ky</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>26</td>\n",
       "      <td>http://reddit.com/r/AskReddit/comments/3h0lm7/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>JosesMagicSchoolBus</td>\n",
       "      <td>49</td>\n",
       "      <td>He's already our greatest ever manager.\\n\\nI t...</td>\n",
       "      <td>0</td>\n",
       "      <td>1438978347000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>t1_ctv6r51</td>\n",
       "      <td>soccer</td>\n",
       "      <td>49</td>\n",
       "      <td>http://reddit.com/r/soccer/comments/3g5kx7/c/c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>notdoingwork</td>\n",
       "      <td>21</td>\n",
       "      <td>Maybe I'm going to come to your house and murd...</td>\n",
       "      <td>0</td>\n",
       "      <td>1439832504000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>t1_cu62ify</td>\n",
       "      <td>cringepics</td>\n",
       "      <td>21</td>\n",
       "      <td>http://reddit.com/r/cringepics/comments/3h9e2d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>946</th>\n",
       "      <td>CaptainJackRyan</td>\n",
       "      <td>75</td>\n",
       "      <td>Disgusting. \\n\\nGreat job OP</td>\n",
       "      <td>0</td>\n",
       "      <td>1439161409000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>t1_ctxcgql</td>\n",
       "      <td>DestinyTheGame</td>\n",
       "      <td>75</td>\n",
       "      <td>http://reddit.com/r/DestinyTheGame/comments/3g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>947</th>\n",
       "      <td>Monstermart</td>\n",
       "      <td>13</td>\n",
       "      <td>Our short passing is fucking beautiful today. ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1439052588000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>t1_ctw10ny</td>\n",
       "      <td>soccer</td>\n",
       "      <td>13</td>\n",
       "      <td>http://reddit.com/r/soccer/comments/3g929h/c/c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948</th>\n",
       "      <td>llshuxll</td>\n",
       "      <td>45</td>\n",
       "      <td>Blizzard is targeting the right people, you ar...</td>\n",
       "      <td>0</td>\n",
       "      <td>1439406581000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>t1_cu0pgcm</td>\n",
       "      <td>heroesofthestorm</td>\n",
       "      <td>45</td>\n",
       "      <td>http://reddit.com/r/heroesofthestorm/comments/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949</th>\n",
       "      <td>rawrnnn</td>\n",
       "      <td>429</td>\n",
       "      <td>&amp;gt; goes to the gym\\n\\n&amp;gt; does 1 set of 1pl...</td>\n",
       "      <td>0</td>\n",
       "      <td>1438557573000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>t1_ctpbvsi</td>\n",
       "      <td>4chan</td>\n",
       "      <td>429</td>\n",
       "      <td>http://reddit.com/r/4chan/comments/3fjnrl/c/ct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>BeautifulBeardy</td>\n",
       "      <td>20</td>\n",
       "      <td>*Laces out* is shouted in unison by all my NFL...</td>\n",
       "      <td>0</td>\n",
       "      <td>1438880964000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>t1_cttrdn2</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>20</td>\n",
       "      <td>http://reddit.com/r/AskReddit/comments/3g0atu/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951</th>\n",
       "      <td>audersaur</td>\n",
       "      <td>27</td>\n",
       "      <td>I have a KVD lipstick that matches my lips. I ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1438881534000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>t1_cttrsdz</td>\n",
       "      <td>MakeupAddiction</td>\n",
       "      <td>27</td>\n",
       "      <td>http://reddit.com/r/MakeupAddiction/comments/3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>UglyMuffins</td>\n",
       "      <td>12</td>\n",
       "      <td>Honestly Cubs fans - it looks like you guys ar...</td>\n",
       "      <td>0</td>\n",
       "      <td>1440160846000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>t1_cuakx6n</td>\n",
       "      <td>baseball</td>\n",
       "      <td>12</td>\n",
       "      <td>http://reddit.com/r/baseball/comments/3hswxj/c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>Conejodc</td>\n",
       "      <td>13</td>\n",
       "      <td>They never should have partnered with Hooli.</td>\n",
       "      <td>0</td>\n",
       "      <td>1438762764000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>t1_cts3ztu</td>\n",
       "      <td>MMA</td>\n",
       "      <td>13</td>\n",
       "      <td>http://reddit.com/r/MMA/comments/3fsvp2/c/cts3ztu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>Archon1111</td>\n",
       "      <td>21</td>\n",
       "      <td>So I have decided to print some information of...</td>\n",
       "      <td>0</td>\n",
       "      <td>1440484625000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>t1_cuetwg8</td>\n",
       "      <td>Drugs</td>\n",
       "      <td>21</td>\n",
       "      <td>http://reddit.com/r/Drugs/comments/3i9k2f/c/cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955</th>\n",
       "      <td>IAmNotJoshua</td>\n",
       "      <td>23</td>\n",
       "      <td>Yeah. Just look at................\\n\\n...\\n\\n....</td>\n",
       "      <td>0</td>\n",
       "      <td>1440756425000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>t1_cuigyqy</td>\n",
       "      <td>BlackPeopleTwitter</td>\n",
       "      <td>23</td>\n",
       "      <td>http://reddit.com/r/BlackPeopleTwitter/comment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956</th>\n",
       "      <td>patjs92</td>\n",
       "      <td>18</td>\n",
       "      <td>I only recognize the latter of that statement.</td>\n",
       "      <td>0</td>\n",
       "      <td>1439942494000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>t1_cu7o91g</td>\n",
       "      <td>nba</td>\n",
       "      <td>18</td>\n",
       "      <td>http://reddit.com/r/nba/comments/3hhs9b/c/cu7o91g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>957</th>\n",
       "      <td>lurh</td>\n",
       "      <td>40</td>\n",
       "      <td>Maybe it's good that she's so abrasive. Keeps ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1439942878000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>t1_cu7ogxh</td>\n",
       "      <td>AskWomen</td>\n",
       "      <td>40</td>\n",
       "      <td>http://reddit.com/r/AskWomen/comments/3hgx07/c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958</th>\n",
       "      <td>uncomfortablyhigh</td>\n",
       "      <td>28</td>\n",
       "      <td>We ended up giving the shirt to our Logistics ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1440625599000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>t1_cugrg5s</td>\n",
       "      <td>CFB</td>\n",
       "      <td>28</td>\n",
       "      <td>http://reddit.com/r/CFB/comments/3iigki/c/cugrg5s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959</th>\n",
       "      <td>EllenPaoFUPA</td>\n",
       "      <td>105</td>\n",
       "      <td>Great job media! You stirred enough shit to co...</td>\n",
       "      <td>0</td>\n",
       "      <td>1440625600000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>t1_cugrg6s</td>\n",
       "      <td>news</td>\n",
       "      <td>105</td>\n",
       "      <td>http://reddit.com/r/news/comments/3ihyj6/c/cug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>ShadyHighlander</td>\n",
       "      <td>15</td>\n",
       "      <td>/r/wrestlewiththeplot is leaking.</td>\n",
       "      <td>0</td>\n",
       "      <td>1441059928000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>t1_cumcktx</td>\n",
       "      <td>SquaredCircle</td>\n",
       "      <td>15</td>\n",
       "      <td>http://reddit.com/r/SquaredCircle/comments/3j4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>961</th>\n",
       "      <td>Bender22</td>\n",
       "      <td>1158</td>\n",
       "      <td>It's how you are supposed to carry [them.](htt...</td>\n",
       "      <td>0</td>\n",
       "      <td>1439393504000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>t1_cu0g9mr</td>\n",
       "      <td>movies</td>\n",
       "      <td>1158</td>\n",
       "      <td>http://reddit.com/r/movies/comments/3gpyl6/c/c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>PutYourLilHandInMine</td>\n",
       "      <td>15</td>\n",
       "      <td>Lol Lorenzo. \"I enjoy watching you train.. It'...</td>\n",
       "      <td>0</td>\n",
       "      <td>1438477074000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>t1_ctof5m7</td>\n",
       "      <td>MMA</td>\n",
       "      <td>15</td>\n",
       "      <td>http://reddit.com/r/MMA/comments/3fg7l6/c/ctof5m7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963</th>\n",
       "      <td>wyattrulesherp</td>\n",
       "      <td>41</td>\n",
       "      <td>Stevie G got a goal in his opener</td>\n",
       "      <td>0</td>\n",
       "      <td>1439192450000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>t1_ctxr1zv</td>\n",
       "      <td>soccer</td>\n",
       "      <td>41</td>\n",
       "      <td>http://reddit.com/r/soccer/comments/3gev4h/c/c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964</th>\n",
       "      <td>tylerbird</td>\n",
       "      <td>15</td>\n",
       "      <td>But you're not suppose to get high off your ow...</td>\n",
       "      <td>0</td>\n",
       "      <td>1440310134000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>t1_cucl7ig</td>\n",
       "      <td>trees</td>\n",
       "      <td>15</td>\n",
       "      <td>http://reddit.com/r/trees/comments/3i11xr/c/cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>965</th>\n",
       "      <td>EastenNinja</td>\n",
       "      <td>39</td>\n",
       "      <td>I wouldn't say deserved to the extent it is.\\n...</td>\n",
       "      <td>0</td>\n",
       "      <td>1439934527000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>t1_cu7jl28</td>\n",
       "      <td>trees</td>\n",
       "      <td>39</td>\n",
       "      <td>http://reddit.com/r/trees/comments/3hgl3h/c/cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>966</th>\n",
       "      <td>antiproton</td>\n",
       "      <td>23</td>\n",
       "      <td>&amp;gt; it isn't itself western influence\\n\\nYou'...</td>\n",
       "      <td>0</td>\n",
       "      <td>1439934555000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>t1_cu7jlmy</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>23</td>\n",
       "      <td>http://reddit.com/r/worldnews/comments/3hhile/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967</th>\n",
       "      <td>coopstar777</td>\n",
       "      <td>87</td>\n",
       "      <td>I mean, we could use a little more fact and a ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1440130135000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>t1_cuabg5c</td>\n",
       "      <td>pics</td>\n",
       "      <td>87</td>\n",
       "      <td>http://reddit.com/r/pics/comments/3hsjni/c/cua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>968</th>\n",
       "      <td>ALLSTARTRIPOD</td>\n",
       "      <td>41</td>\n",
       "      <td>It's quite hard to find the words to describe ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1440755484000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>t1_cuigquo</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>41</td>\n",
       "      <td>http://reddit.com/r/AskReddit/comments/3ipibt/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>969</th>\n",
       "      <td>DavisWuhu</td>\n",
       "      <td>41</td>\n",
       "      <td>Are you sure you don't want to make a bracket ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1439522768000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>t1_cu2ehk7</td>\n",
       "      <td>anime</td>\n",
       "      <td>41</td>\n",
       "      <td>http://reddit.com/r/anime/comments/3gx52q/c/cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>marmosetohmarmoset</td>\n",
       "      <td>16</td>\n",
       "      <td>Especially since chromosomes aren't even a ful...</td>\n",
       "      <td>0</td>\n",
       "      <td>1439523529000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>t1_cu2evl4</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>16</td>\n",
       "      <td>http://reddit.com/r/AskReddit/comments/3gw3y4/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>ZachBroChill</td>\n",
       "      <td>20</td>\n",
       "      <td>We have one for almost every B1G school (Sorry...</td>\n",
       "      <td>0</td>\n",
       "      <td>1439491411000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>t1_cu1vjpl</td>\n",
       "      <td>CFB</td>\n",
       "      <td>20</td>\n",
       "      <td>http://reddit.com/r/CFB/comments/3guwk0/c/cu1vjpl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>Serpentus</td>\n",
       "      <td>14</td>\n",
       "      <td>Push the envelope, watch it bend. :)</td>\n",
       "      <td>0</td>\n",
       "      <td>1440026304000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>t1_cu8v84a</td>\n",
       "      <td>television</td>\n",
       "      <td>14</td>\n",
       "      <td>http://reddit.com/r/television/comments/3hl2py...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>ClassicCarLife</td>\n",
       "      <td>34</td>\n",
       "      <td>She also asked for general advice about food a...</td>\n",
       "      <td>0</td>\n",
       "      <td>1438498827000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>t1_ctoodkz</td>\n",
       "      <td>bestof</td>\n",
       "      <td>34</td>\n",
       "      <td>http://reddit.com/r/bestof/comments/3fghiu/c/c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>SH4Z4M</td>\n",
       "      <td>48</td>\n",
       "      <td>Took me way to long to realize it was you repl...</td>\n",
       "      <td>0</td>\n",
       "      <td>1440474190000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>t1_cuepvds</td>\n",
       "      <td>pics</td>\n",
       "      <td>48</td>\n",
       "      <td>http://reddit.com/r/pics/comments/3ia4sq/c/cue...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>phazerbutt</td>\n",
       "      <td>14</td>\n",
       "      <td>Have you ever kung fu chopped someone while yo...</td>\n",
       "      <td>0</td>\n",
       "      <td>1439262765000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>t1_ctyrd2j</td>\n",
       "      <td>IAmA</td>\n",
       "      <td>14</td>\n",
       "      <td>http://reddit.com/r/IAmA/comments/3gixri/c/cty...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>976 rows  11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   author  avg_score  \\\n",
       "0                Keundrum       3877   \n",
       "1                 McBjure         13   \n",
       "2               Egsession         37   \n",
       "3           fresh_prince_         13   \n",
       "4                djohn_14         11   \n",
       "5                MissyPie         30   \n",
       "6                   Nilwx         23   \n",
       "7             Mike_Gainer         78   \n",
       "8                  nopooq         11   \n",
       "9            LutherDingle         11   \n",
       "10            inuyome1008         94   \n",
       "11        cyborgcommando0         45   \n",
       "12              Ekman-ish         26   \n",
       "13   throwmeaway123456780         30   \n",
       "14              clone9786         47   \n",
       "15   Holy_Kamikaze_Batman         98   \n",
       "16                 uprons         16   \n",
       "17              ScousePie         17   \n",
       "18        RedOrmTostesson        106   \n",
       "19             CameraMan1         17   \n",
       "20               dr3blira         26   \n",
       "21               TKDbeast        184   \n",
       "22      BunniesWithRabies         17   \n",
       "23             chocorazor         16   \n",
       "24          Princepurple1         14   \n",
       "25    dontfuckingdennisme        259   \n",
       "26               NecroGod         95   \n",
       "27              choixpeau         26   \n",
       "28    JosesMagicSchoolBus         49   \n",
       "29           notdoingwork         21   \n",
       "..                    ...        ...   \n",
       "946       CaptainJackRyan         75   \n",
       "947           Monstermart         13   \n",
       "948              llshuxll         45   \n",
       "949               rawrnnn        429   \n",
       "950       BeautifulBeardy         20   \n",
       "951             audersaur         27   \n",
       "952           UglyMuffins         12   \n",
       "953              Conejodc         13   \n",
       "954            Archon1111         21   \n",
       "955          IAmNotJoshua         23   \n",
       "956               patjs92         18   \n",
       "957                  lurh         40   \n",
       "958     uncomfortablyhigh         28   \n",
       "959          EllenPaoFUPA        105   \n",
       "960       ShadyHighlander         15   \n",
       "961              Bender22       1158   \n",
       "962  PutYourLilHandInMine         15   \n",
       "963        wyattrulesherp         41   \n",
       "964             tylerbird         15   \n",
       "965           EastenNinja         39   \n",
       "966            antiproton         23   \n",
       "967           coopstar777         87   \n",
       "968         ALLSTARTRIPOD         41   \n",
       "969             DavisWuhu         41   \n",
       "970    marmosetohmarmoset         16   \n",
       "971          ZachBroChill         20   \n",
       "972             Serpentus         14   \n",
       "973        ClassicCarLife         34   \n",
       "974                SH4Z4M         48   \n",
       "975            phazerbutt         14   \n",
       "\n",
       "                                               comment  controversiality  \\\n",
       "0    I don't know why they would possibly tell peop...                 0   \n",
       "1    He wont earn shit because it's against youtube...                 0   \n",
       "2    They weren't dating that long though -- not ev...                 0   \n",
       "3                                                 kek                  0   \n",
       "4    I think DS2's DLC had AMAZING music, especiall...                 0   \n",
       "5    Thank you! ^ ^\\n\\nI'm sorry, my heart belongs ...                 0   \n",
       "6                                          \"Spaceship\"                 0   \n",
       "7    Mastodon: Pendulous Skin\\nhttps://www.youtube....                 0   \n",
       "8    Yes! This happens SO often. Thank you for shar...                 0   \n",
       "9    The article characterizes Trump as an outsider...                 0   \n",
       "10                                         JESUS WEPT!                 0   \n",
       "11                                      Halo 1 pistol.                 0   \n",
       "12   Not a diver, but I'm thinking you wouldn't act...                 0   \n",
       "13   The white community needs to step up and voice...                 0   \n",
       "14   Why do people hate this so much? It's definite...                 0   \n",
       "15                      http://i.imgur.com/rB4Hux7.gif                 0   \n",
       "16                   Any idea if Korean will be added?                 0   \n",
       "17   That was Chelsea, after we sent them a polite ...                 0   \n",
       "18   I actually liked his set. It was pretty fun if...                 0   \n",
       "19                               at yeast you tried...                 0   \n",
       "20   There's a huge pottery fair once a year near m...                 0   \n",
       "21               Astute observation, /u/Israeli_Shill.                 0   \n",
       "22   By design *and* choice here\\n\\nI find it reall...                 0   \n",
       "23         Much more impressive looking at live speed.                 0   \n",
       "24   1. Using the Bear as a Puffin. 2. This is in n...                 0   \n",
       "25   So just drive across the border. Zambia, in Ma...                 0   \n",
       "26   &gt; and start cleaning things.\\n\\nWell, take ...                 0   \n",
       "27   I wonder why they thought a military school wo...                 0   \n",
       "28   He's already our greatest ever manager.\\n\\nI t...                 0   \n",
       "29   Maybe I'm going to come to your house and murd...                 0   \n",
       "..                                                 ...               ...   \n",
       "946                       Disgusting. \\n\\nGreat job OP                 0   \n",
       "947  Our short passing is fucking beautiful today. ...                 0   \n",
       "948  Blizzard is targeting the right people, you ar...                 0   \n",
       "949  &gt; goes to the gym\\n\\n&gt; does 1 set of 1pl...                 0   \n",
       "950  *Laces out* is shouted in unison by all my NFL...                 0   \n",
       "951  I have a KVD lipstick that matches my lips. I ...                 0   \n",
       "952  Honestly Cubs fans - it looks like you guys ar...                 0   \n",
       "953       They never should have partnered with Hooli.                 0   \n",
       "954  So I have decided to print some information of...                 0   \n",
       "955  Yeah. Just look at................\\n\\n...\\n\\n....                 0   \n",
       "956    I only recognize the latter of that statement.                  0   \n",
       "957  Maybe it's good that she's so abrasive. Keeps ...                 0   \n",
       "958  We ended up giving the shirt to our Logistics ...                 0   \n",
       "959  Great job media! You stirred enough shit to co...                 0   \n",
       "960                  /r/wrestlewiththeplot is leaking.                 0   \n",
       "961  It's how you are supposed to carry [them.](htt...                 0   \n",
       "962  Lol Lorenzo. \"I enjoy watching you train.. It'...                 0   \n",
       "963                  Stevie G got a goal in his opener                 0   \n",
       "964  But you're not suppose to get high off your ow...                 0   \n",
       "965  I wouldn't say deserved to the extent it is.\\n...                 0   \n",
       "966  &gt; it isn't itself western influence\\n\\nYou'...                 0   \n",
       "967  I mean, we could use a little more fact and a ...                 0   \n",
       "968  It's quite hard to find the words to describe ...                 0   \n",
       "969  Are you sure you don't want to make a bracket ...                 0   \n",
       "970  Especially since chromosomes aren't even a ful...                 0   \n",
       "971  We have one for almost every B1G school (Sorry...                 0   \n",
       "972               Push the envelope, watch it bend. :)                 0   \n",
       "973  She also asked for general advice about food a...                 0   \n",
       "974  Took me way to long to realize it was you repl...                 0   \n",
       "975  Have you ever kung fu chopped someone while yo...                 0   \n",
       "\n",
       "                 created distinguished  downs        name                subr  \\\n",
       "0    1439958069000000000          None      0  t1_cu7x0ut           worldnews   \n",
       "1    1439230955000000000          None      0  t1_cty7slu              videos   \n",
       "2    1438453918000000000          None      0  t1_cto3vte              movies   \n",
       "3    1439654076000000000          None      0  t1_cu3ync9               DotA2   \n",
       "4    1438713706000000000          None      0  t1_ctrdvzs               Games   \n",
       "5    1440596771000000000          None      0  t1_cug8byb               anime   \n",
       "6    1439068042000000000          None      0  t1_ctw8zz5     GlobalOffensive   \n",
       "7    1440167085000000000          None      0  t1_cuao760               Music   \n",
       "8    1440174263000000000          None      0  t1_cuaswna           AskReddit   \n",
       "9    1438699354000000000          None      0  t1_ctr482t            politics   \n",
       "10   1440736851000000000          None      0  t1_cuibege              gaming   \n",
       "11   1439483902000000000          None      0  t1_cu1qceu           AskReddit   \n",
       "12   1440968757000000000          None      0  t1_cul4aay          askscience   \n",
       "13   1440968967000000000          None      0  t1_cul4ehb                news   \n",
       "14   1439252732000000000          None      0  t1_ctylkk1           AskReddit   \n",
       "15   1438635380000000000          None      0  t1_ctqbxrl                pics   \n",
       "16   1438511347000000000          None      0  t1_ctorarl           AskReddit   \n",
       "17   1439919074000000000          None      0  t1_cu792lo              soccer   \n",
       "18   1439166604000000000          None      0  t1_ctxfay0               DotA2   \n",
       "19   1438408661000000000          None      0  t1_ctnp1sc            woahdude   \n",
       "20   1440357891000000000          None      0  t1_cud2po0   TrollXChromosomes   \n",
       "21   1438629057000000000          None      0  t1_ctq7j45             pokemon   \n",
       "22   1439072227000000000          None      0  t1_ctwaayd           AskReddit   \n",
       "23   1439059232000000000          None      0  t1_ctw4j40              soccer   \n",
       "24   1439230726000000000          None      0  t1_cty7n02       AdviceAnimals   \n",
       "25   1438661150000000000          None      0  t1_ctqqxoc           worldnews   \n",
       "26   1438873990000000000          None      0  t1_cttmisy                 WTF   \n",
       "27   1439620972000000000          None      0  t1_cu3p1ky           AskReddit   \n",
       "28   1438978347000000000          None      0  t1_ctv6r51              soccer   \n",
       "29   1439832504000000000          None      0  t1_cu62ify          cringepics   \n",
       "..                   ...           ...    ...         ...                 ...   \n",
       "946  1439161409000000000          None      0  t1_ctxcgql      DestinyTheGame   \n",
       "947  1439052588000000000          None      0  t1_ctw10ny              soccer   \n",
       "948  1439406581000000000          None      0  t1_cu0pgcm    heroesofthestorm   \n",
       "949  1438557573000000000          None      0  t1_ctpbvsi               4chan   \n",
       "950  1438880964000000000          None      0  t1_cttrdn2           AskReddit   \n",
       "951  1438881534000000000          None      0  t1_cttrsdz     MakeupAddiction   \n",
       "952  1440160846000000000          None      0  t1_cuakx6n            baseball   \n",
       "953  1438762764000000000          None      0  t1_cts3ztu                 MMA   \n",
       "954  1440484625000000000          None      0  t1_cuetwg8               Drugs   \n",
       "955  1440756425000000000          None      0  t1_cuigyqy  BlackPeopleTwitter   \n",
       "956  1439942494000000000          None      0  t1_cu7o91g                 nba   \n",
       "957  1439942878000000000          None      0  t1_cu7ogxh            AskWomen   \n",
       "958  1440625599000000000          None      0  t1_cugrg5s                 CFB   \n",
       "959  1440625600000000000          None      0  t1_cugrg6s                news   \n",
       "960  1441059928000000000          None      0  t1_cumcktx       SquaredCircle   \n",
       "961  1439393504000000000          None      0  t1_cu0g9mr              movies   \n",
       "962  1438477074000000000          None      0  t1_ctof5m7                 MMA   \n",
       "963  1439192450000000000          None      0  t1_ctxr1zv              soccer   \n",
       "964  1440310134000000000          None      0  t1_cucl7ig               trees   \n",
       "965  1439934527000000000          None      0  t1_cu7jl28               trees   \n",
       "966  1439934555000000000          None      0  t1_cu7jlmy           worldnews   \n",
       "967  1440130135000000000          None      0  t1_cuabg5c                pics   \n",
       "968  1440755484000000000          None      0  t1_cuigquo           AskReddit   \n",
       "969  1439522768000000000          None      0  t1_cu2ehk7               anime   \n",
       "970  1439523529000000000          None      0  t1_cu2evl4           AskReddit   \n",
       "971  1439491411000000000          None      0  t1_cu1vjpl                 CFB   \n",
       "972  1440026304000000000          None      0  t1_cu8v84a          television   \n",
       "973  1438498827000000000          None      0  t1_ctoodkz              bestof   \n",
       "974  1440474190000000000          None      0  t1_cuepvds                pics   \n",
       "975  1439262765000000000          None      0  t1_ctyrd2j                IAmA   \n",
       "\n",
       "      ups                                                url  \n",
       "0    3877  http://reddit.com/r/worldnews/comments/3hidw9/...  \n",
       "1      13  http://reddit.com/r/videos/comments/3ggt2v/c/c...  \n",
       "2      37  http://reddit.com/r/movies/comments/3fe8ru/c/c...  \n",
       "3      13  http://reddit.com/r/DotA2/comments/3h30x7/c/cu...  \n",
       "4      11  http://reddit.com/r/Games/comments/3fr6dt/c/ct...  \n",
       "5      30  http://reddit.com/r/anime/comments/3igs0f/c/cu...  \n",
       "6      23  http://reddit.com/r/GlobalOffensive/comments/3...  \n",
       "7      78  http://reddit.com/r/Music/comments/3hu1mf/c/cu...  \n",
       "8      11  http://reddit.com/r/AskReddit/comments/3hu28g/...  \n",
       "9      11  http://reddit.com/r/politics/comments/3fqr96/c...  \n",
       "10     94  http://reddit.com/r/gaming/comments/3io172/c/c...  \n",
       "11     45  http://reddit.com/r/AskReddit/comments/3gv455/...  \n",
       "12     26  http://reddit.com/r/askscience/comments/3iykdi...  \n",
       "13     30  http://reddit.com/r/news/comments/3iyrqb/c/cul...  \n",
       "14     47  http://reddit.com/r/AskReddit/comments/3gi7dv/...  \n",
       "15     98  http://reddit.com/r/pics/comments/3fnb7p/c/ctq...  \n",
       "16     16  http://reddit.com/r/AskReddit/comments/3fgxoz/...  \n",
       "17     17  http://reddit.com/r/soccer/comments/3hgs01/c/c...  \n",
       "18    106  http://reddit.com/r/DotA2/comments/3geei8/c/ct...  \n",
       "19     17  http://reddit.com/r/woahdude/comments/3fc09i/c...  \n",
       "20     26  http://reddit.com/r/TrollXChromosomes/comments...  \n",
       "21    184  http://reddit.com/r/pokemon/comments/3fn2yj/c/...  \n",
       "22     17  http://reddit.com/r/AskReddit/comments/3g8epl/...  \n",
       "23     16  http://reddit.com/r/soccer/comments/3g92pd/c/c...  \n",
       "24     14  http://reddit.com/r/AdviceAnimals/comments/3gg...  \n",
       "25    259  http://reddit.com/r/worldnews/comments/3foob1/...  \n",
       "26     95  http://reddit.com/r/WTF/comments/3fzv89/c/cttmisy  \n",
       "27     26  http://reddit.com/r/AskReddit/comments/3h0lm7/...  \n",
       "28     49  http://reddit.com/r/soccer/comments/3g5kx7/c/c...  \n",
       "29     21  http://reddit.com/r/cringepics/comments/3h9e2d...  \n",
       "..    ...                                                ...  \n",
       "946    75  http://reddit.com/r/DestinyTheGame/comments/3g...  \n",
       "947    13  http://reddit.com/r/soccer/comments/3g929h/c/c...  \n",
       "948    45  http://reddit.com/r/heroesofthestorm/comments/...  \n",
       "949   429  http://reddit.com/r/4chan/comments/3fjnrl/c/ct...  \n",
       "950    20  http://reddit.com/r/AskReddit/comments/3g0atu/...  \n",
       "951    27  http://reddit.com/r/MakeupAddiction/comments/3...  \n",
       "952    12  http://reddit.com/r/baseball/comments/3hswxj/c...  \n",
       "953    13  http://reddit.com/r/MMA/comments/3fsvp2/c/cts3ztu  \n",
       "954    21  http://reddit.com/r/Drugs/comments/3i9k2f/c/cu...  \n",
       "955    23  http://reddit.com/r/BlackPeopleTwitter/comment...  \n",
       "956    18  http://reddit.com/r/nba/comments/3hhs9b/c/cu7o91g  \n",
       "957    40  http://reddit.com/r/AskWomen/comments/3hgx07/c...  \n",
       "958    28  http://reddit.com/r/CFB/comments/3iigki/c/cugrg5s  \n",
       "959   105  http://reddit.com/r/news/comments/3ihyj6/c/cug...  \n",
       "960    15  http://reddit.com/r/SquaredCircle/comments/3j4...  \n",
       "961  1158  http://reddit.com/r/movies/comments/3gpyl6/c/c...  \n",
       "962    15  http://reddit.com/r/MMA/comments/3fg7l6/c/ctof5m7  \n",
       "963    41  http://reddit.com/r/soccer/comments/3gev4h/c/c...  \n",
       "964    15  http://reddit.com/r/trees/comments/3i11xr/c/cu...  \n",
       "965    39  http://reddit.com/r/trees/comments/3hgl3h/c/cu...  \n",
       "966    23  http://reddit.com/r/worldnews/comments/3hhile/...  \n",
       "967    87  http://reddit.com/r/pics/comments/3hsjni/c/cua...  \n",
       "968    41  http://reddit.com/r/AskReddit/comments/3ipibt/...  \n",
       "969    41  http://reddit.com/r/anime/comments/3gx52q/c/cu...  \n",
       "970    16  http://reddit.com/r/AskReddit/comments/3gw3y4/...  \n",
       "971    20  http://reddit.com/r/CFB/comments/3guwk0/c/cu1vjpl  \n",
       "972    14  http://reddit.com/r/television/comments/3hl2py...  \n",
       "973    34  http://reddit.com/r/bestof/comments/3fghiu/c/c...  \n",
       "974    48  http://reddit.com/r/pics/comments/3ia4sq/c/cue...  \n",
       "975    14  http://reddit.com/r/IAmA/comments/3gixri/c/cty...  \n",
       "\n",
       "[976 rows x 11 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dt_conversion(timestamp):\n",
    "    return datetime.fromtimestamp(timestamp/ 1000000000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import TimestampType\n",
    "import pyspark.sql.functions as func\n",
    "from datetime import datetime\n",
    "\n",
    "dt_conv = func.udf(dt_conversion, TimestampType())\n",
    "new_df = parquet.withColumn('created_h', dt_conv(parquet.created))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+\n",
      "|            created|           created_h|\n",
      "+-------------------+--------------------+\n",
      "|1439958069000000000|2015-08-19 07:21:...|\n",
      "|1439958091000000000|2015-08-19 07:21:...|\n",
      "|1439776080000000000|2015-08-17 04:48:...|\n",
      "|1439776681000000000|2015-08-17 04:58:...|\n",
      "|1439533443000000000|2015-08-14 09:24:...|\n",
      "|1439533453000000000|2015-08-14 09:24:...|\n",
      "|1440175759000000000|2015-08-21 19:49:...|\n",
      "|1439230955000000000|2015-08-10 21:22:...|\n",
      "|1439231094000000000|2015-08-10 21:24:...|\n",
      "|1440518065000000000|2015-08-25 18:54:...|\n",
      "|1439747290000000000|2015-08-16 20:48:...|\n",
      "|1439747532000000000|2015-08-16 20:52:...|\n",
      "|1439747786000000000|2015-08-16 20:56:...|\n",
      "|1439118706000000000|2015-08-09 14:11:...|\n",
      "|1439119438000000000|2015-08-09 14:23:...|\n",
      "|1438453918000000000|2015-08-01 21:31:...|\n",
      "|1439285611000000000|2015-08-11 12:33:...|\n",
      "|1439286052000000000|2015-08-11 12:40:...|\n",
      "|1439286791000000000|2015-08-11 12:53:...|\n",
      "|1439855123000000000|2015-08-18 02:45:...|\n",
      "+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.select('created', 'created_h').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL query example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-c7e6dbf503e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m f = urllib.urlretrieve (\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\", \n\u001b[0;32m----> 3\u001b[0;31m                     \"kddcup.data_10_percent.gz\")\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pritykovskaya/anaconda/lib/python2.7/urllib.pyc\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data, context)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_urlopener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreporthook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0murlcleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_urlopener\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pritykovskaya/anaconda/lib/python2.7/urllib.pyc\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self, url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    271\u001b[0m                     \u001b[0mreporthook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocknum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m                     \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pritykovskaya/anaconda/lib/python2.7/socket.pyc\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    382\u001b[0m                 \u001b[0;31m# fragmentation issues on many platforms.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mEINTR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "f = urllib.urlretrieve (\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\", \n",
    "                    \"kddcup.data_10_percent.gz\")\n",
    "\n",
    "\n",
    "data_file = \"./kddcup.data_10_percent.gz\"\n",
    "raw_data = sc.textFile(data_file).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "csv_data = raw_data.map(lambda l: l.split(\",\"))\n",
    "row_data = csv_data.map(lambda p: Row(\n",
    "    duration=int(p[0]), \n",
    "    protocol_type=p[1],\n",
    "    service=p[2],\n",
    "    flag=p[3],\n",
    "    src_bytes=int(p[4]),\n",
    "    dst_bytes=int(p[5])\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dst_bytes: long (nullable = true)\n",
      " |-- duration: long (nullable = true)\n",
      " |-- flag: string (nullable = true)\n",
      " |-- protocol_type: string (nullable = true)\n",
      " |-- service: string (nullable = true)\n",
      " |-- src_bytes: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "interactions_df = sqlCtx.createDataFrame(row_data)\n",
    "interactions_df.registerTempTable(\"interactions\")\n",
    "interactions_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|duration|dst_bytes|\n",
      "+--------+---------+\n",
      "|    5057|        0|\n",
      "|    5059|        0|\n",
      "|    5051|        0|\n",
      "|    5056|        0|\n",
      "|    5051|        0|\n",
      "|    5039|        0|\n",
      "|    5062|        0|\n",
      "|    5041|        0|\n",
      "|    5056|        0|\n",
      "|    5064|        0|\n",
      "|    5043|        0|\n",
      "|    5061|        0|\n",
      "|    5049|        0|\n",
      "|    5061|        0|\n",
      "|    5048|        0|\n",
      "|    5047|        0|\n",
      "|    5044|        0|\n",
      "|    5063|        0|\n",
      "|    5068|        0|\n",
      "|    5062|        0|\n",
      "+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select tcp network interactions with more than 1 second duration and no transfer from destination\n",
    "tcp_interactions = sqlCtx.sql(\"\"\"\n",
    "    SELECT duration, dst_bytes FROM interactions WHERE protocol_type = 'tcp' AND \n",
    "    duration > 1000 AND dst_bytes = 0\n",
    "\"\"\")\n",
    "tcp_interactions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[duration: bigint, dst_bytes: bigint]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tcp_interactions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'interactions_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-d8d25c478cfa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0minteractions_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"protocol_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"duration\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dst_bytes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"protocol_type\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'interactions_df' is not defined"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "t0 = time()\n",
    "interactions_df.select(\"protocol_type\", \"duration\", \"dst_bytes\").groupBy(\"protocol_type\").count().show()\n",
    "tt = time() - t0\n",
    "\n",
    "print \"Query performed in {} seconds\".format(round(tt,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-b0a3ff1eaabb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m {interactions_df.select(\"protocol_type\", \"duration\", \"dst_bytes\")\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minteractions_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mduration\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minteractions_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdst_bytes\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "\n",
    "{interactions_df.select(\"protocol_type\", \"duration\", \"dst_bytes\")\n",
    "    .filter(interactions_df.duration>1000)\n",
    "    .filter(interactions_df.dst_bytes==0)\n",
    "    .groupBy(\"protocol_type\")\n",
    "    .count()\n",
    "    .show()\n",
    "}  \n",
    "tt = time() - t0\n",
    "\n",
    "print \"Query performed in {} seconds\".format(round(tt,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_label_type(label):\n",
    "    if label!=\"normal.\":\n",
    "        return \"attack\"\n",
    "    else:\n",
    "        return \"normal\"\n",
    "\n",
    "row_labeled_data = csv_data.map(lambda p: Row(\n",
    "    duration=int(p[0]), \n",
    "    protocol_type=p[1],\n",
    "    service=p[2],\n",
    "    flag=p[3],\n",
    "    src_bytes=int(p[4]),\n",
    "    dst_bytes=int(p[5]),\n",
    "    label=get_label_type(p[41])\n",
    "    )\n",
    ")\n",
    "interactions_labeled_df = sqlCtx.createDataFrame(row_labeled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### GraphFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from graphframes import * \n",
    "\n",
    "vertices = sqlCtx.createDataFrame([\n",
    "  (\"1\", \"Alex\", 28, \"M\"),\n",
    "  (\"2\", \"Emeli\", 28, \"F\"),\n",
    "  (\"3\", \"Natasha\", 27, \"F\"),\n",
    "  (\"4\", \"Pavel\", 30, \"M\"),\n",
    "  (\"5\", \"Oleg\", 35, \"M\"),\n",
    "  (\"6\", \"Ivan\", 30, \"M\"),\n",
    "  (\"7\", \"Ilya\", 29, \"M\")], \n",
    "    [\"id\", \"name\", \"age\", \"gender\"])\n",
    "\n",
    "edges = sqlCtx.createDataFrame([\n",
    "  (\"1\", \"2\", \"friend\"),\n",
    "  (\"2\", \"1\", \"friend\"),\n",
    "  (\"1\", \"3\", \"friend\"),\n",
    "  (\"3\", \"1\", \"friend\"),\n",
    "  (\"1\", \"4\", \"friend\"),\n",
    "  (\"4\", \"1\", \"friend\"),\n",
    "  (\"2\", \"3\", \"friend\"), \n",
    "  (\"3\", \"2\", \"friend\"),\n",
    "  (\"2\", \"5\", \"friend\"),\n",
    "  (\"5\", \"2\", \"friend\"),\n",
    "  (\"3\", \"4\", \"friend\"),\n",
    "  (\"4\", \"3\", \"friend\"),\n",
    "  (\"3\", \"5\", \"friend\"),\n",
    "  (\"5\", \"3\", \"friend\"),\n",
    "  (\"3\", \"6\", \"friend\"),\n",
    "  (\"6\", \"3\", \"friend\"),\n",
    "  (\"3\", \"7\", \"friend\"),\n",
    "  (\"7\", \"3\", \"friend\")\n",
    "], [\"src\", \"dst\", \"relationship\"])\n",
    "\n",
    "g = GraphFrame(vertices, edges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+------+\n",
      "| id|name|age|gender|\n",
      "+---+----+---+------+\n",
      "|  5|Oleg| 35|     M|\n",
      "+---+----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g.vertices.filter(\"age > 30\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|inDegree|\n",
      "+---+--------+\n",
      "|  3|       6|\n",
      "|  1|       3|\n",
      "|  2|       3|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g.inDegrees.filter(\"inDegree > 2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------+----------------+------------+\n",
      "|               a|           e|               b|          e2|\n",
      "+----------------+------------+----------------+------------+\n",
      "|   [1,Alex,28,M]|[1,4,friend]|  [4,Pavel,30,M]|[4,1,friend]|\n",
      "|  [4,Pavel,30,M]|[4,1,friend]|   [1,Alex,28,M]|[1,4,friend]|\n",
      "|[3,Natasha,27,F]|[3,2,friend]|  [2,Emeli,28,F]|[2,3,friend]|\n",
      "|  [2,Emeli,28,F]|[2,1,friend]|   [1,Alex,28,M]|[1,2,friend]|\n",
      "|  [2,Emeli,28,F]|[2,5,friend]|   [5,Oleg,35,M]|[5,2,friend]|\n",
      "|[3,Natasha,27,F]|[3,5,friend]|   [5,Oleg,35,M]|[5,3,friend]|\n",
      "|   [1,Alex,28,M]|[1,3,friend]|[3,Natasha,27,F]|[3,1,friend]|\n",
      "|[3,Natasha,27,F]|[3,1,friend]|   [1,Alex,28,M]|[1,3,friend]|\n",
      "|   [5,Oleg,35,M]|[5,3,friend]|[3,Natasha,27,F]|[3,5,friend]|\n",
      "|  [2,Emeli,28,F]|[2,3,friend]|[3,Natasha,27,F]|[3,2,friend]|\n",
      "|[3,Natasha,27,F]|[3,7,friend]|   [7,Ilya,29,M]|[7,3,friend]|\n",
      "|  [4,Pavel,30,M]|[4,3,friend]|[3,Natasha,27,F]|[3,4,friend]|\n",
      "|   [6,Ivan,30,M]|[6,3,friend]|[3,Natasha,27,F]|[3,6,friend]|\n",
      "|   [1,Alex,28,M]|[1,2,friend]|  [2,Emeli,28,F]|[2,1,friend]|\n",
      "|[3,Natasha,27,F]|[3,4,friend]|  [4,Pavel,30,M]|[4,3,friend]|\n",
      "+----------------+------------+----------------+------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g.find(\"(a)-[e]->(b); (b)-[e2]->(a)\").show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+----------------+\n",
      "|               A|               B|               C|\n",
      "+----------------+----------------+----------------+\n",
      "|   [7,Ilya,29,M]|[3,Natasha,27,F]|   [7,Ilya,29,M]|\n",
      "|   [5,Oleg,35,M]|[3,Natasha,27,F]|   [7,Ilya,29,M]|\n",
      "|   [6,Ivan,30,M]|[3,Natasha,27,F]|   [7,Ilya,29,M]|\n",
      "|   [1,Alex,28,M]|[3,Natasha,27,F]|   [7,Ilya,29,M]|\n",
      "|  [4,Pavel,30,M]|[3,Natasha,27,F]|   [7,Ilya,29,M]|\n",
      "|  [2,Emeli,28,F]|[3,Natasha,27,F]|   [7,Ilya,29,M]|\n",
      "|[3,Natasha,27,F]|   [7,Ilya,29,M]|[3,Natasha,27,F]|\n",
      "|[3,Natasha,27,F]|   [5,Oleg,35,M]|[3,Natasha,27,F]|\n",
      "|  [2,Emeli,28,F]|   [5,Oleg,35,M]|[3,Natasha,27,F]|\n",
      "|[3,Natasha,27,F]|   [6,Ivan,30,M]|[3,Natasha,27,F]|\n",
      "|[3,Natasha,27,F]|   [1,Alex,28,M]|[3,Natasha,27,F]|\n",
      "|  [4,Pavel,30,M]|   [1,Alex,28,M]|[3,Natasha,27,F]|\n",
      "|  [2,Emeli,28,F]|   [1,Alex,28,M]|[3,Natasha,27,F]|\n",
      "|[3,Natasha,27,F]|  [4,Pavel,30,M]|[3,Natasha,27,F]|\n",
      "|   [1,Alex,28,M]|  [4,Pavel,30,M]|[3,Natasha,27,F]|\n",
      "+----------------+----------------+----------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "motifs = g.find(\"(A)-[]->(B); (B)-[]->(C)\")\n",
    "motifs.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+----------------+\n",
      "|             A|               B|               C|\n",
      "+--------------+----------------+----------------+\n",
      "| [5,Oleg,35,M]|[3,Natasha,27,F]|   [7,Ilya,29,M]|\n",
      "| [6,Ivan,30,M]|[3,Natasha,27,F]|   [7,Ilya,29,M]|\n",
      "| [1,Alex,28,M]|[3,Natasha,27,F]|   [7,Ilya,29,M]|\n",
      "|[4,Pavel,30,M]|[3,Natasha,27,F]|   [7,Ilya,29,M]|\n",
      "|[2,Emeli,28,F]|[3,Natasha,27,F]|   [7,Ilya,29,M]|\n",
      "|[2,Emeli,28,F]|   [5,Oleg,35,M]|[3,Natasha,27,F]|\n",
      "|[4,Pavel,30,M]|   [1,Alex,28,M]|[3,Natasha,27,F]|\n",
      "|[2,Emeli,28,F]|   [1,Alex,28,M]|[3,Natasha,27,F]|\n",
      "| [1,Alex,28,M]|  [4,Pavel,30,M]|[3,Natasha,27,F]|\n",
      "| [5,Oleg,35,M]|  [2,Emeli,28,F]|[3,Natasha,27,F]|\n",
      "| [1,Alex,28,M]|  [2,Emeli,28,F]|[3,Natasha,27,F]|\n",
      "| [7,Ilya,29,M]|[3,Natasha,27,F]|   [5,Oleg,35,M]|\n",
      "| [6,Ivan,30,M]|[3,Natasha,27,F]|   [5,Oleg,35,M]|\n",
      "| [1,Alex,28,M]|[3,Natasha,27,F]|   [5,Oleg,35,M]|\n",
      "|[4,Pavel,30,M]|[3,Natasha,27,F]|   [5,Oleg,35,M]|\n",
      "+--------------+----------------+----------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "motifs = g.find(\"(A)-[]->(B); (B)-[]->(C)\").filter(\"A.id != C.id\")\n",
    "motifs.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-------+---+------+\n",
      "|count| id|   name|age|gender|\n",
      "+-----+---+-------+---+------+\n",
      "|    0|  7|   Ilya| 29|     M|\n",
      "|    3|  3|Natasha| 27|     F|\n",
      "|    1|  5|   Oleg| 35|     M|\n",
      "|    0|  6|   Ivan| 30|     M|\n",
      "|    2|  1|   Alex| 28|     M|\n",
      "|    1|  4|  Pavel| 30|     M|\n",
      "|    2|  2|  Emeli| 28|     F|\n",
      "+-----+---+-------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = g.triangleCount()\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+------+----------+--------+\n",
      "| id|   name|age|gender|university|pagerank|\n",
      "+---+-------+---+------+----------+--------+\n",
      "|  1|   Alex| 28|     M|      MIPT|    0.15|\n",
      "|  3|Natasha| 27|     F|     SPbSU|    0.15|\n",
      "|  2|  Emeli| 28|     F|      MIPT| 0.21375|\n",
      "|  4|  Pavel| 30|     M|      MIPT| 0.21375|\n",
      "|  7|   Ilya| 29|     M|       MSU|  0.1925|\n",
      "|  6|   Ivan| 30|     M|       MSU|  0.1925|\n",
      "|  5|   Oleg| 35|     M|      MIPT|  0.1925|\n",
      "+---+-------+---+------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = g.pageRank(resetProbability=0.15, maxIter=10)\n",
    "results.vertices.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### new graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vertices = sqlCtx.createDataFrame([\n",
    "  (\"1\", \"Alex\", 28, \"M\", \"MIPT\"),\n",
    "  (\"2\", \"Emeli\", 28, \"F\", \"MIPT\"),\n",
    "  (\"3\", \"Natasha\", 27, \"F\", \"SPbSU\"),\n",
    "  (\"4\", \"Pavel\", 30, \"M\", \"MIPT\"),\n",
    "  (\"5\", \"Oleg\", 35, \"M\", \"MIPT\"),\n",
    "  (\"6\", \"Ivan\", 30, \"M\", \"MSU\"),\n",
    "  (\"7\", \"Ilya\", 29, \"M\", \"MSU\")], [\"id\", \"name\", \"age\", \"gender\", \"university\"])\n",
    "\n",
    "edges = sqlCtx.createDataFrame([\n",
    "  (\"1\", \"2\", \"friend\"),\n",
    "  (\"1\", \"4\", \"friend\"),\n",
    "  (\"3\", \"5\", \"friend\"),\n",
    "  (\"3\", \"6\", \"friend\"),\n",
    "  (\"3\", \"7\", \"friend\")\n",
    "], [\"src\", \"dst\", \"type\"])\n",
    "\n",
    "g = GraphFrame(vertices, edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+\n",
      "| id|    component|\n",
      "+---+-------------+\n",
      "|  3|  25769803776|\n",
      "|  7|  25769803776|\n",
      "|  5|  25769803776|\n",
      "|  6|  25769803776|\n",
      "|  2|1236950581248|\n",
      "|  4|1236950581248|\n",
      "|  1|1236950581248|\n",
      "+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sc.setCheckpointDir(\"/tmp\")\n",
    "result = g.connectedComponents()\n",
    "result.select(\"id\", \"component\").orderBy(\"component\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
